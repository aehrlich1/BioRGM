batch_size: 32,
dim_h: 128 # should be inferred from pretrain model
datasets:
  - "BACE"
  - "BBBP"
dropout: 0.1 #  [0.1, 0.3, 0.5]
epochs: 10 # [50, 100, 200]
freeze_pretrain: True # [False, True] # True | False
lr: 0.0005 # [0.0005, 0.0001]
pretrain_models:
  - "silver-fox-42"
  - "rising-sunshine-36"
  - "chemberta-77M"
  - "None"
runs: 3
task: "finetune"